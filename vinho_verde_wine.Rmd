---
title: "WHAT MAKES PORTUGUESE VINHO VERDE WINE FINE?"
author: "Siyabonga Mabuza"
date: "12/15/2021"
output:
  word_document: default
  html_document: default
  pdf_document: default
toc: yes
---

# Abstract 

What makes fine wine? The red wine variant of the Portuguese "Vinho Verde" wine refers to Portuguese wine that originated in the historic Minho province in the far north of the country. The main goal of this investigation is to find which features of these kinds of wine provide the most information about its quality.

This investigation aims to answer the following questions:

- What physiochemical quality is the most important to making good quality wine?

- What role does the level of alcohol have in the quality of wine

- Does less residual sugar mean less quality?

This dataset contains one categorical response variable (High or Poor) quality wine and 12 physicochemical predictor variables. The 12 physicochemical predictor variables are: Fixed Acidity, Volatile Acidity, Citric Acid, Residual Sugar, Chlorides, Free Sulfur Dioxide, Total Sulfur Dioxide,  Density,  pH, Sulphates, and Alcohol. 

The response variable is Quality. It was originally ranked on a scale of 1-10 with 1 through 5 being wine of poor quality and 6 through 10 being wine high quality. We changed this to 1 for high quality and 0 for poor quality. There are 1599 wine variants.

Our final model had an AUC of 84% prediction for quality of wine explained by the predictors of our final model. Thus, our model is significant in predicting the quality of wine. Log of Alcohol was the most significant predictor in the final model so interestingly, wines with higher alcohol contents have higher quality. Furthermore, residual sugar was one of the first predictors removed using stepwise regression. It seems that residual sugar has no significance towards the quality of wine. 

# Data Characteristics

```{r}
library(readxl)
winequality <- read_excel("C:/Users/Siyabonga Mabuza/Desktop/DataSets/wine.xlsx")
wine.data = winequality
names(wine.data) [1] = "quality"
attach(wine.data)
head(wine.data)
```

First, we will examine the characteristics of the response variable.

```{r}
library(vcd)
library(MASS)
barplot (table (ifelse ( quality ==1, "Yes", "No")), xlab="Wine Quality is Good",
ylab="Number of Wine Variants", main = "Wine Quality Barplot")
```

This dataset has a sample size of size of 1599 wine variants. Within this sample, 855 of the wine variants have good quality and 744 do not have good quality. Based on this, no transformation is required for the response variable.

Now we will examine the characteristics of the predictor variables.

```{r}
library(ggplot2)
library(tidyr)
hist(alcohol)
hist(log(alcohol))
```

Alcohol is right skewed, a log transformation slightly alleviates this. Thus, we will use a log transformation for alcohol. 

```{r}
hist(`fixed acidity`)
hist(log(`fixed acidity`))
```

Fixed acidity is right skewed. A log transformation resolves this, thus, we will use a log transformation for fixed acidity. 

```{r}
hist(`volatile acidity`)
hist(sqrt(`volatile acidity`))
```

Volatile Acidity is slightly rightly skewed. A square root transformation alleviates this. Thus, we will use a square root transformation for Volatile Acidity. 

```{r}
hist(`citric acid`)
hist(sqrt(`citric acid`))
```

Citric Acid is rightly skewed, a square root transformation slightly alleviates this. Thus, we will make a square root transformation for Citric Acid. 

```{r}
hist(`residual sugar`)
hist(log(`residual sugar`))
```

Residual Sugar is rightly skewed, a log transformation alleviates this. Thus, we will use a log transformation for residual sugar. 

```{r}
hist(chlorides)
hist(log(chlorides))
```

Chlorides is rightly skewed, a log transformation resolves this. Thus, we will use a log transformation for Chlorides.

```{r}
hist(`free sulfur dioxide`)
hist(log(`free sulfur dioxide`))
```

Free Sulfur Dioxide is rightly skewed, a log transformation resolves this. Thus we will use a log transformation for Free Sulfur Dioxide. 

```{r}
hist(`total sulfur dioxide`)
hist(log(`total sulfur dioxide`))
```

Total Sulfur Dioxide is rightly skewed, a log transformation resolves this. Thus, Total Sulfur Dioxide will be log transformed. 

```{r}
hist(density)
```

No transformation is necessary for density. 

```{r}
hist(pH)
```

No transformation is necessary for pH. 

```{r}
hist(sulphates)
hist(log(sulphates))
```

Sulphates is right skewed, a log transformation alleviates this. Thus, a log transformation will be used for sulphates. 

# Scatter Plot matrix and Correlations

We will now create a scatterplot and a correlation plot with coefficients in order to check for any linear association between the variables. 

```{r}
wine.data$log.sulphates = log(sulphates)
wine.data$log.totalsulfur.dioxide = log(`total sulfur dioxide`)
wine.data$log.freesulfur.dioxide = log(`free sulfur dioxide`)
wine.data$log.chlorides = log(chlorides)
wine.data$log.residual.sugar = log(`residual sugar`)
wine.data$sqrt.citric.acid = sqrt(`citric acid`)
wine.data$sqrt.volatile.acidity = sqrt(`volatile acidity`)
wine.data$log.fixed.acidity = log(`fixed acidity`)
wine.data$log.alcohol = log(alcohol)

plot(wine.data [, c(1, 9:10, 13:21)])
```

```{r}
transcorr = cor(wine.data [, c(1, 9:10, 13:21)], use= "complete.obs")
library("corrplot")
## corrplot 0.84 loaded
corrplot (transcorr, method = "number", number.cex=0.6)
```

Log Free Sulfur Dioxide and Log Total Sulfur Dioxide have a relatively high positive correlation (0.78). This is expected because the Total sulfur dioxide measures free and bound forms of Sulfur Dioxide. Density and log Fixed Acidity are reasonably positively correlated(0.67). pH and log Fixed Acidity have a relatively high negative correlation (0.67). 

There are no variables that have extremely high positive or negative correlations. However, there are numerous variables with low positive and negative correlations. 

#  First Order Logistic Regression Model

We now fit a logistic regression model that includes all of the variables and their transformations.

```{r}
fit1 = glm (data=wine.data, quality ~ log(alcohol) + log(`fixed acidity`) + sqrt(`volatile acidity`) + sqrt(`citric acid`) + log(`residual sugar`) + log(chlorides) + log(`free sulfur dioxide`) + log(`total sulfur dioxide`) + log(sulphates) + pH + density, family = "binomial")
summary(fit1)
exp (fit1$coefficients)
exp (confint (fit1))
```

Upon fitting the first order model, there are eight variables that are significant. These variables are:

+ log of alcohol
+ log of fixed acidity
+ square root of volatile.acidity
+ square root of citric.acid
+ log of chlorides
+ log of free sulfur dioxide
+ log of total sulfur dioxide
+ log of sulphates  

Log of free sulfur dioxide and log of total sulfur dioxide which also happen to have the highest correlation between the variables are the two most significant variables. 

# Jittered Response vs. Predicted Values Plot

Next we will plot the jittered response vs. predicted values.

```{r}
pred.logit = predict (fit1)
plot(jitter(quality,0.1) ~ pred.logit, data = wine.data, main = 'Jittered Response vs. Predicted Values Plot', xlab="Predicted Logit", ylab="Failure, Observed and Probability")
pred.r = predict(fit1,type='response')
pred.order =order(pred.logit)
lines(pred.logit[pred.order],pred.r[pred.order],col='blue')
lines(lowess(pred.logit[pred.order], pred.r[pred.order]),col='red',lty=2)
```

Initially, there seems to be a higher density of good quality and not good quality as the predicted logit value of the model increases. However, once the predicted logit value surpasses two, there seems to be a higher density of good quality. 

# Residuals vs Fitted Plot

We will now examine the residuals vs fitted plot.

```{r}
plot(fit1, which=1)
```

Overall, the residual distribution from our model appears to have little deviation from zero, so the distribution of the residuals is not concerning. There are some obvious outliers, particularly point 653. 

# Variance Inflaction Factors, Cook's distance, and High Leverage

First, we will check for multicollinearity. 

```{r}
car:: vif (fit1)
```

Log of fixed acidity and density have relatively high has VIF values (They are above the cautionary value of 5), 7.88 and 6.81, respectfully. However, because this is not extremely high we will keep these variables. 

Next, we will examine leverage and Cook's distance. 

```{r}
plot(fit1, which=5)
abline(v=0.0225, col="blue")
```

There do not seem to be any points with high cook's distance. The high leverage cutoff point is 3(11+1)/1599=0.0225. There are numerous points with high leverage. This is concerning. 

# Model Selection

We will now fit the initial model using AIC stepwise regression.

```{r}
fit2= step(fit1, direction = "both")
```

Upon fitting the model using stepwise regression, eight variables are worth keeping. Namely:

+ log of alcohol
+ log of fixed acidity
+ square root of volatile.acidity
+ square root of citric.acid
+ log of chlorides
+ log of free sulfur dioxide
+ log of total sulfur dioxide
+ log of sulphates 

We will now use fit this model using stepwise regression with the BIC criterion.

```{r}
n = dim (wine.data)[1]
fit3=step(fit2, direction='both', k=log(n))
```

Log of chlorides was removed from the model using the BIC criterion. Thus, the best model that can be derived from this first stepwise model is quality ~ log of alcohol + log of fixed acidity + square root of volatile.acidity + square root of citric.acid + log of free sulfur dioxide + log of total sulfur dioxide + log of sulphates. 

# Two Way Interaction effects

Now, we will check for interaction effects. We will be using the seven predictors kept in the last model. 

```{r}
my.center = function (x) (x - mean (x))
logfreesulfurdioxide=my.center(log(`free sulfur dioxide`))
logfixedacidity=my.center(log(`fixed acidity`))
sqrtcitricacid=my.center(sqrt(`citric acid`))
logtotalsulfurdioxide=my.center(log(`total sulfur dioxide`))
logsulphates=my.center(log(sulphates))
sqrtvolatileacidity=my.center(sqrt(`volatile acidity`))
logalcohol=my.center(log(alcohol))
fit4 = glm (quality ~ ( logfreesulfurdioxide + logfixedacidity + sqrtcitricacid + logtotalsulfurdioxide +  logsulphates + sqrtvolatileacidity + logalcohol)^2, family = "binomial")
summary(fit4)

```

There are five significant interactions. Namely:

+ log of free sulfur dioxide: log of total sulfur dioxide
+ log of free sulfur dioxide: log of alcohol
+ log of fixed acidity: square root of citric acid
+ square root of citric acid: log of sulphates
+ log of total sulfur dioxide: log of sulphates

# Model Selection on Interaction Model

We now use stepwise regression on the interaction model, fit4, to see if there is a model with interaction terms that fits better than fit3.

```{r}
fit5=step(fit4, direction="both")
```
```{r}
summary(fit5)
```

Using stepwise regression, 10 interaction effects were significant. These are:

- log of fixed acidity:log of alcohol                  
- log of free sulfur dioxide:square root of volatile acidity    
- log of fixed acidity:log of sulphates            
- log of free sulfur dioxide:log of fixed acidity       
- log of free sulfur dioxide:log of alcohol           
- log of total sulfur dioxide:square root of volatile acidity   
- square root of citric acid:log of sulphates               
- log of fixed acidity:square root of citric acid              
- log of total sulfur dioxide:log of sulphates          
- log of free sulfur dioxide:log of total sulfur dioxide

We will therefore use these interactions for our final model as well as the significant predictors from the previous model in our final model.

Now, we will plot two of the interaction effects and what these interactions mean.

```{r}
categorize = function (x, quantiles=(1:3)/4) {
  cutoffs = quantile (x, quantiles)
  n.cutoffs = length (cutoffs)
  result = rep ("C1", length (x))
  for (j in 1:n.cutoffs) {
    result [x > cutoffs [j]] = paste ("C", j+1, sep="")
  }
  return (result)
}
library (ggplot2)
library (dplyr)

with (wine.data, qplot (x=logfixedacidity, y=predict(fit5), color=categorize (logalcohol)) +
geom_smooth (method="lm"))

```

From the interaction plot of log fixed acidity and log of alcohol we can see a negative linear relationship between the two. The interaction between acidity and alcohol is negative – as alcohol increases, the slope between quality and acidity decreases. 

```{r}
with (wine.data, qplot (x=logfreesulfurdioxide, y=predict(fit5), color=categorize (sqrtvolatileacidity)) +
geom_smooth (method="lm"))
```

Similar comment as above: As volatile acidity increases (C1 to C4), the slope between quality and free sulfur dioxide increases (The C4 slope is positive, while the others are negative and about the same).

# Final Model

```{r}
summary(fit5)
```
```{r}
# Odds ratios
#logfreesulfurdioxide
exp (0.5121)
#logfixedacidity 
exp (1.8466)
#sqrtcitricacid 
exp (-1.7743)
#logtotalsulfurdioxide
exp (-0.9154)
#logsulphates 
exp (2.5583)
#sqrtvolatileacidity
exp (-4.9766)
#logalcohol
exp (10.4915)
exp(confint(fit5))
```

All the predictors in our final model are significant, the AIC is 1616 while the previous model (fit4) had an AIC of 1679.6. The parameter estimates, standard errors, and p-values are as follows:

Log free sulfur dioxide has an estimate of 0.5121, standard error of 0.1521 and a p-value of 0.000757

Log fixed acidity has an estimate of 1.8466, standard error of 0.4704, and a p-value of 8.67e-05

Square root of citric acid has an estimate of -1.7743, a standard error of 0.4627, and a p-value of 0.000126

Log total sulfur dioxide has an estimate of -0.9154, a standard error of 0.1590, and a p-value of 8.54e-09

Log sulphates has an estimate of 2.5583, an estimate of 0.3363, and a p-value of 2.78e-14

Square root of volatile acidity has an estimate of -4.9766, a standard error of 0.7175, and a p-value of 4.04e-12 log of alcohol has an estimate of 10.4915, a standard error of 0.8160, and a p-value of < 2e-16


Next we will investigate some model diagnostics.

## Model Diagnostics 

## Likelihood ratio test for final model and Goodness of fit test

```{r}
# Likelihood ratio test for final model
pchisq(fit5$null.deviance - fit5$deviance, fit5$df.null - fit5$df.residual, lower=F)
# Goodness of fit test
pchisq (deviance(fit5), df.residual(fit5), lower=F)
```
 
The likelihood ratio test has a p-value of 1.034e-122, this means that we have significant predictors. 

The goodness of fit test has a p-value of 0.5024012 which is very high and greater that 0.05 thus we can conclude that the model likely fits.

## Residual Analysis & Diagnostic Analysis

```{r}
par(mfrow = c(1,2)) 
plot(fit5, which=c(1,5))
```

In the residual analysis vs fitted analysis has the smoothing line at 0 thus showing good linearity, points 1430 and 653 deviate a bit far from the smoothing line but still follow the curve.

The residuals vs leverage shows point 653 has large negative residual, otherwise the residuals are generally clustered to the left with one point at the right making the line go upwards a bit. 

## Scatter Plot for Final Model

```{r}

transcorr = cor(wine.data [, c(1, 9:10, 13:21)], use= "complete.obs")
library("corrplot")
## corrplot 0.84 loaded
corrplot (transcorr, method = "number", number.cex=0.6)
```

The high leverage values do not seem too influential.

## Model Plot

```{r}
pred.logit = predict (fit5)
plot(jitter(quality,0.1) ~ pred.logit, data = wine.data, main = 'Jittered Response vs. Predicted Values Plot', xlab="Predicted Logit", ylab="Failure, Observed and Probability")
pred.r = predict(fit5,type='response')
pred.order =order(pred.logit)
lines(pred.logit[pred.order],pred.r[pred.order],col='blue')
lines(lowess(pred.logit[pred.order], pred.r[pred.order]),col='red',lty=2)
```

The final model plot shows that there is a concentration of “low quality” responses when the predicted logit value of the model is low and a concentration of “high quality” responses when the logit value is high. This shows the model has an ability to predict better than the first
one. 

## Influence Analysis

```{r}
levcut = 2*(sqrt(4/1599)) 
levcut
library(car)
influenceIndexPlot(fit5)
```

Our hat matrix leverage cutoff (2*sqrt(p/n)) is 0.1000313 The diagnostic plot shows that only one case exceeds to this cutoff value, as case 3 has the highest leverage at roughly 0.15, this was seen in the cooks distance with one value indicating to have high cooks distance. 

## ROC Curve

```{r}
par (mfrow=c(1,1)) 
library(ROCR)
pred1 <- prediction(fit5$fitted.values, fit5$y)
perf1 <- performance(pred1,"tpr","fpr")
auc1 <- performance(pred1,"auc")@y.values[[1]]
auc1
```
```{r}
plot(perf1, lwd=2, col=2)
abline(0,1)
legend(0.6, 0.3, c(paste ("AUC=", round (auc1, 4), sep="")), lwd=2, col=2)
roc.x = slot (perf1, "x.values") [[1]]
roc.y = slot (perf1, "y.values") [[1]]
cutoffs = slot (perf1, "alpha.values") [[1]]
auc.table = cbind.data.frame(cutoff=pred1@cutoffs,
tp=pred1@tp, fp=pred1@fp, tn=pred1@tn,
fn=pred1@fn)
names (auc.table) = c("Cutoff", "TP", "FP", "TN", "FN")
auc.table$sensitivity = auc.table$TP / (auc.table$TP + auc.table$FN)
auc.table$specificity = auc.table$TN / (auc.table$TN + auc.table$FP)
auc.table$FalsePosRate = 1 - auc.table$specificity
auc.table$sens_spec = auc.table$sensitivity + auc.table$specificity
auc.best = auc.table [auc.table$sens_spec == max (auc.table$sens_spec),]
auc.best
```

We got an AUC of 0.84 thus the model correctly predicted 84% of the data with a specificity of 0.71 and sensitivity of 0.82.
The cutoff is 0.44 with a false positive rate of 0.29. This makes us confident this model is our best model to use.

## Conclusions

Conclusively, having gone through all the process and the necessary steps to find our best model that is reliably predictable, our final model is “fit5”. All seven variables in the model are significant, thus, based on all the processes, these predictors, build our model to predict quality of wine. 

Additionally, using the stepwise regression on the centered interaction effect between multiple variables, the model shows that the following had significant interactions:

- log of free sulfur dioxide: log of total sulfur dioxide
- log of free sulfur dioxide: log of alcohol
- log of fixed acidity: square root of citric acid
- square root of citric acid: log of sulphates
- log of total sulfur dioxide: log of sulphates

We also got an AUC of 84% prediction for quality of wine explained by the predictors of our final model. 

With all these results we found that our model would be significant in predicting the quality of wine. Log of Alcohol was the most significant predictor in the final model so interestingly, wines with higher alcohol contents have higher quality. Furthermore, residual sugar was one of the first predictors removed using stepwise regression. It seems that residual sugar has no significance towards the quality of wine. 

The model from a physiochemical lens seems to give conclusive evidence to what makes a good quality wine. However, if we had other factors considered such as types of grapes used, how long the wine was aged for, location (considering the effects of weather), etc (Which were not provided in the dataset). A better model could have been produced and we would have been able to examine the impact of these aforementioned factors to the quality of Vinho Verde wine.   

